# 神经网络设计
- 介绍一下神经网络的初始化方法
  - 零初始化（Zero Initialization）
    - 方法: 所有权重初始化为零，偏置也初始化为零。
    - 缺点: 如果所有权重都初始化为零，那么在反向传播时每个神经元的梯度将会相同，导致它们在更新时完全同步。这使得所有神经元都学到相同的特征，无法发挥深度学习的优势。因此，零初始化不适用于权重的初始化。
  - 随机初始化（Random Initialization）
    - 方法: 以随机值初始化权重，通常使用均匀分布或正态分布的随机数。
    - 应用: 这种方法常用于简单的神经网络，但对于深层网络，随机初始化容易导致梯度消失或梯度爆炸问题。
    - 缺点: 随机选择的分布和范围对网络的收敛速度和稳定性有很大影响，不当的随机初始化会导致训练效率低下。
  - Xavier 初始化（Glorot Initialization）
    - 方法: Xavier 初始化通过正态分布或均匀分布随机初始化权重，其方差依赖于前一层的输入神经元数量。具体来说，如果使用正态分布，均值为0，方差为1/n，其中n是前一层的神经元数量。如果使用均匀分布，则权重从区间[-sqrt(6/n), sqrt(6/n)]中抽取。
    - 优点: Xavier 初始化通过平衡输入和输出的方差，防止信号在前向传播时过快消失或膨胀，适合用于 sigmoid 和 tanh 激活函数的网络。
    - 应用: 常用于多层感知机（MLP）和卷积神经网络（CNN）。
  - He 初始化（He Initialization）
    - 方法: He 初始化是 Xavier 初始化的改进版，专为 ReLU 和其变体激活函数设计。正态分布的均值为0，方差为2/n，其中n是前一层的神经元数量。对于均匀分布，权重从区间[-sqrt(6/n), sqrt(6/n)]中抽取。
    - 优点: He 初始化考虑到 ReLU 激活函数会将部分输入置为零，因此通过增加方差来增强前向传播中的信号。
    - 应用: 广泛用于深层卷积神经网络和其他使用 ReLU 激活函数的网络。
  - Lecun 初始化
    - 方法: Lecun 初始化方法与 He 初始化类似，但方差设为1/n，其中n为前一层的神经元数量。该初始化更适合 sigmoid 和 tanh 激活函数。
    - 应用: 特别适用于使用 sigmoid 激活函数的神经网络，如 LSTM 和其他递归神经网络（RNN）。
  - Orthogonal 初始化（Orthogonal Initialization）
    - 方法: 通过生成随机矩阵并使用正交分解（如 QR 分解）来初始化权重矩阵，使得矩阵之间保持正交性。权重矩阵的特征值保持不变，且其所有列均正交。
    - 优点: 正交初始化可以帮助深层网络避免梯度消失或梯度爆炸问题。
    - 应用: 常用于 RNN 和深度卷积网络中，特别是在网络非常深或序列特别长的情况下。
  - 正态分布与均匀分布初始化（Normal and Uniform Initialization）
    - 方法: 权重初始化为从正态分布或均匀分布中抽取的随机值。均匀分布通常用作 Xavier 初始化，而正态分布则用于 He 初始化等。
    - 应用: 可根据网络的具体需求调整分布的均值和方差。
  - 自适应初始化（Adaptive Initialization）
    - 方法: 在一些先进的神经网络架构中，初始化可以是自适应的，根据输入数据动态调整初始权重。例如，在 Batch Normalization 或 Layer Normalization 中，层的参数会在训练过程中自动适应和调整。
  - 实际应用中的注意事项
    - 激活函数: 初始化方法通常与激活函数相关联，不同的激活函数可能需要不同的初始化方法。例如，ReLU 常与 He 初始化配合使用，而 sigmoid 和 tanh 常与 Xavier 初始化配合。
    - 网络深度: 随着网络的深度增加，初始化的选择变得更为关键。深层网络容易出现梯度消失或梯度爆炸问题，因此需要更仔细地选择初始化方法。
    - 批归一化（Batch Normalization）: 使用批归一化可以在一定程度上缓解对初始化的依赖，但仍需注意初始权重的适当选择。

- 介绍一下神经网络的优化器有哪些。
  - 梯度下降法（Gradient Descent）
    - 批量梯度下降（Batch Gradient Descent）: 计算整个训练集上的损失梯度，使用该梯度更新参数。这种方法通常需要较多的计算资源，且内存占用较大，但收敛较稳定。
    - 随机梯度下降（Stochastic Gradient Descent, SGD）: 每次使用一个样本计算梯度并更新参数，更新频繁，计算量小，但由于每次只基于一个样本更新，可能导致训练过程中损失波动较大。
    - 小批量梯度下降（Mini-batch Gradient Descent）: 结合了批量梯度下降和随机梯度下降的优点，每次使用一个小批量的数据计算梯度。这种方法在计算效率和收敛稳定性上取得了平衡，是当前最常用的优化器之一。
  - 动量法（Momentum）
    - 动量梯度下降（SGD with Momentum）: 在标准 SGD 的基础上，引入了动量的概念。动量方法通过在更新时保留之前的梯度，使得优化过程更平滑，能够更快地收敛到最优解，并有助于避免局部最优解。
  - RMSProp（Root Mean Square Propagation）
    - RMSProp 是为了解决 Adagrad（另一种自适应学习率优化器）在训练过程中学习率不断降低的问题。它通过引入指数加权移动平均（Exponential Moving Average, EMA），在每次更新参数时调节学习率，从而使得模型在训练后期仍能保持较快的学习速度。
  - Adam（Adaptive Moment Estimation）
    - Adam 是目前最为流行的优化器之一，结合了 RMSProp 和动量法的优点。它通过同时计算梯度的一阶矩估计（动量）和二阶矩估计（RMSProp），并根据这些估计值来调整每个参数的学习率。Adam 在许多应用场景中表现优异，特别是在处理稀疏梯度和高维数据时。
  - Adadelta
    - Adadelta 是 Adagrad 的改进版，旨在解决 Adagrad 的学习率单调下降问题。与 RMSProp 类似，它利用窗口中梯度的均方根来调整学习率，但不同的是，Adadelta 是无学习率（learning rate-free）的，通过更新步长的动态调整避免了学习率手动设置的问题。
  - Nesterov 加速梯度（Nesterov Accelerated Gradient, NAG）
    - NAG 是对动量法的改进。与标准动量法不同，NAG 在计算当前梯度之前先对未来的梯度方向进行预测，然后根据这个预测值进行更新。这样做可以在更早的时间纠正参数更新的方向，从而进一步加快收敛速度。
  - AdaMax
    - AdaMax 是 Adam 的一个变体，基于无穷范数（infinity norm）来更新参数。它在某些情况下比 Adam 更稳定，尤其是在非常高维的数据集上。
  - Nadam
    - Nadam 是将 Nesterov 加速梯度与 Adam 结合的一种优化器，试图在提升 Adam 的收敛速度的同时，保留其原有的稳定性。
  - L-BFGS（Limited-memory Broyden–Fletcher–Goldfarb–Shanno）
    - L-BFGS 是一种准牛顿法优化器，适用于小型数据集和高精度需求的任务。由于内存需求较高，计算复杂，L-BFGS 较少用于大规模神经网络，但在一些特定场景中可以达到更好的收敛效果。
  - 选择优化器的考虑因素
    - 数据规模和复杂度: 小型数据集通常适合 L-BFGS 之类的高精度优化器，而大规模数据集更适合 SGD 或 Adam 这类快速收敛的优化器。
    - 模型类型: 卷积神经网络（CNN）、循环神经网络（RNN）等不同类型的网络可能对不同优化器有不同的需求。
    - 硬件资源: 如计算能力和内存限制，影响了优化器的选择。

- 对Embedding的理解
  - Embedding 的核心思想是将高维稀疏数据转换为低维稠密向量。
  - 包括：word embedding，句子embedding，图embedding， 用户商品embedding
  - 尽管 Embedding 能有效捕捉数据的内在结构，但其可解释性往往较差

- sigmoid
  - 二分类任务输出层
- softmax
  - 将一个n维的输入向量转换为一个概率分布
  - 可以作为多分类任务输出层


