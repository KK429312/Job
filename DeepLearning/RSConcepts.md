# 推荐系统 - 基本概念
- 介绍一下推荐算法的链路流程
- 特征工程怎么做
  - 特征构建： 主要包括对用户、物品和上下文信息的挖掘与组合：
    - 用户特征：包括用户的基本信息（如年龄、性别、地理位置）、行为特征（如浏览历史、点击率、购买记录）以及社交关系（如好友关系、关注的人）。
    - 物品特征：包括物品的基本属性（如类别、品牌、价格）、流行度（如总购买量、评价数）以及内容特征（如电影的导演、演员、题材）。
    - 上下文特征：包括时间特征（如一天中的时段、周末与工作日）、地理位置特征（用户当前的位置）、设备特征（如移动端或PC端）。
      
  - 特征提取： 特征提取是指从原始数据中提取出能够反映用户行为和偏好的特征，常用的方法包括：
    - 统计特征：从历史数据中计算如点击次数、购买次数、评价分数等统计量。
    - 时间序列特征：通过分析用户行为的时间序列数据提取趋势或周期性特征。
    - 嵌入特征：通过模型（如Word2Vec、DeepWalk等）将用户、物品或其他实体映射到低维向量空间。
    - 交互特征：结合用户和物品特征，通过交叉特征或二阶交互项来捕捉复杂的非线性关系。
      - 交叉特征：可以是离散特征的拼接或组合。例如，将用户的职业和居住城市拼接成“职业-城市”这样的新特征。这种组合在特征空间中形成了新的类别，使得模型可以独立对这些类别进行学习。
      - 二阶交互特征：主要是数值特征的乘积，如用户年龄与商品价格的乘积，或是嵌入向量的内积。这种方式能够捕捉到线性模型无法表达的非线性关系
        -  实现二阶交互的模型： 
          - 因子分解机（Factorization Machines, FM）：FM模型通过内积操作捕捉到特征之间的二阶交互，并且能够有效处理稀疏矩阵问题，适用于推荐系统中的用户-物品交互建模。
          - 深度因子分解机（DeepFM）：在因子分解机的基础上加入了深度神经网络部分，能够捕捉到更高阶、更复杂的交互特征。这种模型可以同时学习低阶和高阶的特征交互，兼顾了模型的准确性和复杂性。
          - Wide & Deep 模型：由Google提出的Wide & Deep模型结合了线性模型（Wide部分）和深度神经网络（Deep部分），Wide部分负责学习低阶的交互特征，而Deep部分则负责学习高阶的非线性交互。这种模型在广告点击率预测和推荐系统中得到了广泛应用。
      
  - 特征选择： 为了提高模型的训练效率和预测效果，需要选择最具信息量的特征
    - 过滤法：通过统计方法（如方差选择法, 相关系数法, 互信息法）对特征进行评分，选择得分高的特征。
      - 皮尔逊相关系数:两个变量之间的线性相关程度。它计算的是两个变量之间协方差与其标准差的比值![img.png](images/img.png)<br>协方差是衡量两个随机变量之间关系的统计量
      - 互信息：基于信息论的方法，衡量两个变量之间共享的信息量，能够捕捉更复杂的非线性关系![img.png](images/img2.png)<br>其中，p(x,y) 是联合概率分布，p(x) 和  p(y) 是边缘概率分布。
    - 包裹法：通过模型（如递归特征消除RFE, Recursive Feature Elimination）来选择对模型效果影响最大的特征。
    - 嵌入法：利用模型的权重或者特征重要性度量（如随机森林的特征重要性、L1正则化等）来选择特征。
      - 随机森林:通过对决策树内部节点分裂所带来的不纯度减少量进行统计，生成特征重要性。
      - 随机森林:不纯度是用来衡量数据集混杂程度的指标。常用的不纯度指标包括基尼系数和熵
      - 随机森林:计算该特征在所有节点分裂中导致的不纯度减少总和，得到该特征在这棵树中的重要性
      - L1正则化: 通过增加所有系数的绝对值之和作为惩罚项，促使模型在训练过程中倾向于让某些系数w_i变为零
      - L1正则化: 使得模型最终选择少量具有显著影响的特征，而将不重要的特征对应的系数压缩为零，从而实现特征选择
    - 降维的方法：PCA，解释方差比例选择成分
      
  - 确认特征重要性： 特征重要性可以帮助理解模型，并进一步优化特征工程过程。常用的方法包括：
    - 基于模型的方法：利用树模型（如随机森林、梯度提升树）的特征重要性指标，或利用线性模型的系数值。
    - 基于置换的重要性评估：通过随机打乱某个特征的数据来观察模型性能的变化，变化越大说明该特征越重要。
      - 随机打乱，该特征的值将与原始样本不再对应，即破坏了该特征与目标变量之间的关联性
    - SHAP值：SHAP（SHapley Additive exPlanations）值能够解释每个特征对模型预测结果的贡献，适用于各种复杂模型。
      - SHAP的核心思想是将模型的预测结果看作是特征“合作”的结果，每个特征对最终预测结果的贡献通过其Shapley值来衡量。
      
  - 实际应用中的考虑
    - 特征交互与非线性：在实际应用中，用户和物品的交互往往是非线性的。可以考虑使用如FM（因子分解机）或DeepFM等模型来自动学习高阶交互特征。
    - 在线与离线特征更新：推荐系统需要应对实时性需求，因此部分特征（如用户的最新行为）需要在线更新，而长期稳定的特征（如用户基本信息）可以离线处理。
    - 特征漂移：随着时间推移，特征的分布可能发生变化（特征漂移）。需要持续监控和调整特征，以保持推荐系统的效果。


- 模型如何进行在线训练，和离线训练有什么区别
  - 在线训练过程：数据流，处理，增量训练，模型实时更新以及备份，实时监控性能 
  - 离线优点
    - 高稳定性：由于在离线环境中进行，整个训练过程受外界影响较小，模型效果较为稳定。
    - 灵活性：可以使用复杂的模型和大量的数据，训练时间较长但可以获得高精度的模型。
    - 易于调试：离线训练有充足的时间进行调试和优化。
  - 离线缺点：更新不及时，时效性不足，数据量依赖，离线训练依赖于大批量数据集，在处理冷启动问题或小数据集时，效果可能不佳。
  - 在线优点：在线训练是指模型在接收到新数据时立即进行更新，而不是定期批量训练。在线训练使得模型能够持续适应新的数据输入，实时更新模型参数。
    - 实时性强，动态适应， 数据效率高：在线训练对数据的使用更为高效，每条新数据都直接用于模型更新，无需等待大批量数据的收集。
  - 在线缺点：复杂度高，稳定性风险，资源消耗大

- 如何保持嵌入一致性的问题
  - 在不同模型版本、训练阶段或系统运行的不同时间点生成的嵌入（embedding）向量，在语义和数值上需要保持一致，以确保模型在推理时的稳定性和预测结果的可靠性。
  - 模型更新带来的不一致性, 分布式训练中的不一致性, 在线/离线环境差异
  - 怎么做：
    - 使用Procrustes Analysis对新旧模型生成的嵌入进行对齐，确保两个嵌入空间保持相似的几何结构。 
    - 映射层：在新模型的嵌入输出上应用一个线性映射层，使其与旧模型的嵌入空间对齐。这种方法尤其在NLP中有应用，通过映射使新旧嵌入向量的语义和几何结构保持一致。
    - 锚点策略：在每次模型更新时，选取一组固定的样本作为锚点，确保这些锚点的嵌入向量在新旧模型中保持相对稳定。通过监控这些锚点的变化，可以判断并控制嵌入一致性。
    - 约束学习：在模型训练时加入约束，要求新模型的嵌入不能偏离旧模型嵌入太远。这个策略可以通过在损失函数中加入一个约束项来实现，使得新的嵌入向量与旧的嵌入向量保持相似。






