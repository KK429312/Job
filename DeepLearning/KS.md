- 特征工程怎么做
  - 特征构建： 主要包括对用户、物品和上下文信息的挖掘与组合：
    - 用户特征：包括用户的基本信息（如年龄、性别、地理位置）、行为特征（如浏览历史、点击率、购买记录）以及社交关系（如好友关系、关注的人）。
    - 物品特征：包括物品的基本属性（如类别、品牌、价格）、流行度（如总购买量、评价数）以及内容特征（如电影的导演、演员、题材）。
    - 上下文特征：包括时间特征（如一天中的时段、周末与工作日）、地理位置特征（用户当前的位置）、设备特征（如移动端或PC端）。
  - 特征提取： 特征提取是指从原始数据中提取出能够反映用户行为和偏好的特征，常用的方法包括：
    - 统计特征：从历史数据中计算如点击次数、购买次数、评价分数等统计量。
    - 时间序列特征：通过分析用户行为的时间序列数据提取趋势或周期性特征。
    - 嵌入特征：通过模型（如Word2Vec、DeepWalk等）将用户、物品或其他实体映射到低维向量空间。
    - 交互特征：结合用户和物品特征，通过交叉特征或二阶交互项来捕捉复杂的非线性关系。
  - 特征选择： 为了提高模型的训练效率和预测效果，需要选择最具信息量的特征
    - 过滤法：通过统计方法（如方差选择法、卡方检验、相关系数法）对特征进行评分，选择得分高的特征。
    - 包裹法：通过模型（如递归特征消除RFE）来选择对模型效果影响最大的特征。
    - 嵌入法：利用模型的权重或者特征重要性度量（如随机森林的特征重要性、L1正则化等）来选择特征。
  - 确认特征重要性： 特征重要性可以帮助理解模型，并进一步优化特征工程过程。常用的方法包括：
    - 基于模型的方法：利用树模型（如随机森林、梯度提升树）的特征重要性指标，或利用线性模型的系数值。
    - 基于置换的重要性评估：通过随机打乱某个特征的数据来观察模型性能的变化，变化越大说明该特征越重要。
    - SHAP值：SHAP（SHapley Additive exPlanations）值能够解释每个特征对模型预测结果的贡献，适用于各种复杂模型。
  - 实际应用中的考虑
    - 特征交互与非线性：在实际应用中，用户和物品的交互往往是非线性的。可以考虑使用如FM（因子分解机）或DeepFM等模型来自动学习高阶交互特征。
    - 在线与离线特征更新：推荐系统需要应对实时性需求，因此部分特征（如用户的最新行为）需要在线更新，而长期稳定的特征（如用户基本信息）可以离线处理。
    = 特征漂移：随着时间推移，特征的分布可能发生变化（特征漂移）。需要持续监控和调整特征，以保持推荐系统的效果。

- 什么是FM和FFM
- self-attention
- transformer
- 如何保持嵌入一致性的问题
- 针对高活用户和低活用户，计算UAUC会有问题，高活用户的 UAUC值比较置信，而低活用户的UAUC值不置信，该如何处理？WUAUC对序列模型是否了解？
- 开放性问题，优化观看时长，怎么设计模型？针对label的设置
- 为什么self-attention可以堆叠多层，有什么作用？ 
- 多头有什么作用？如果想让不同头之间有交互，可以怎么做？
- 讲一讲多目标优化，MMoE怎么设计？如果权重为1,0,0这样全部集中在某一个专家上该怎么办？
- 介绍一下神经网络的优化器有哪些。
- 介绍一下推荐算法的链路流程
- 介绍一下神经网络的初始化方法
- 讲一讲推荐算法序列建模的模型
- 交叉熵推导
- sigmoid和softmax
- 对Embedding的理解
- LSTM
- 解释AUC
- DeepFM
- 为什么Deep部分和wide部分要共享embedding输入，不共享有什么好处和坏处
- 为什么FM适合高维稀疏特征
- 对推荐系统有什么了解
- 模型如何进行在线训练，和离线训练有什么区别
- XGBoost 的特征重要性是如何得到的？
- XGBoost 是如何分裂的
- 随机森林和 lightgbm 的一些参数
- 随机森林的每剪枝
- - word2vec
- COBW - 预测中心词
- Skip-gram - 预测context
