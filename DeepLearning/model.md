- transformer
- 为什么self-attention可以堆叠多层，有什么作用？ 
  - 层之间的独立性：每一层 Self-Attention 都在处理上一层的输出，因此它可以根据更高层次的信息重新计算注意力权重，而不会直接依赖于输入数据的具体内容。这使得 Self-Attention 层能够堆叠，因为每一层都可以逐步提取更高层次的特征。
  - 线性变换与非线性激活： Self-Attention 通常伴随着线性变换和非线性激活（如 ReLU），这使得即使相同的注意力机制在多层中反复应用，模型也能逐渐学习到更加复杂的表示。
  - 作用：
    - 提高模型的表达能力：捕捉复杂的长程依赖，多层次特征提取
    - 信息聚合与扩散：逐层聚合信息，丰富语境表示
    - 提升模型的非线性表达能力
    - 减少低层次信息的影响：逐步提取高层次特征：低层次特征（如局部的、具体的模式）逐渐被抽象和整合为高层次特征（如全局的、抽象的模式）
    
- 多头有什么作用？如果想让不同头之间有交互，可以怎么做？
  - 作用：
    - 捕捉多样化的特征：多个子空间的注意力，丰富的信息表示
    - 提高模型的鲁棒性：减少信息丢失，避免过拟合：多个头的独立计算有助于模型的正则化效果，防止模型在某一个特定的子空间过拟合。
    - 增强模型的表示能力
  - 如何交互
    - 使用门控机制：通过设计类似 LSTM 中的门控机制，结合不同头的输出，通过学习的门控单元动态调整各个头输出的重要性。这种机制可以让模型在不同头之间动态选择或融合，从而实现头间交互。
    - 引入头间交互的权重矩阵
    - 自注意力中的交互机制：1. 多头间的级联处理：让某些头的输出作为其他头的输入，即在不同的头之间建立顺序关系，使得这些头能够以某种顺序逐步细化输入的特征表示。 2. 交互注意力：在每个头计算注意力得分之前，可以先让不同头的查询、键、值矩阵进行一定的线性组合或加权求和，创造一种“交互注意力”。这可以在计算注意力权重之前将头之间的关系编码进来。
    - 交互的组合层：引入一个额外的“组合层”或“融合层”
    
- 什么是FM

- 什么是FFM
- 为什么FM适合高维稀疏特征
- DeepFM
- Wide & Deep
- 为什么Deep部分和wide部分要共享embedding输入，不共享有什么好处和坏处
- DIN
- 讲一讲推荐算法序列建模的模型
- LSTM
- XGBoost 的特征重要性是如何得到的？
- XGBoost 是如何分裂的
- 随机森林和 lightgbm 的一些参数
- 随机森林的每剪枝
- word2vec
- COBW - 预测中心词
- Skip-gram - 预测context